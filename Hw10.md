![image-20220424175638703](C:\Users\hutter_sadan\AppData\Roaming\Typora\typora-user-images\image-20220424175638703.png)

![image-20220424175645312](C:\Users\hutter_sadan\AppData\Roaming\Typora\typora-user-images\image-20220424175645312.png)

![image-20220424175655666](C:\Users\hutter_sadan\AppData\Roaming\Typora\typora-user-images\image-20220424175655666.png)

![image-20220424175705192](C:\Users\hutter_sadan\AppData\Roaming\Typora\typora-user-images\image-20220424175705192.png)

### 10_2

#### (1) 请由仿真结果计算得到𝑥1与𝑥2的皮尔森相关系数

10000次仿真结果的平均值是：

![image-20220424142103972](C:\Users\hutter_sadan\AppData\Roaming\Typora\typora-user-images\image-20220424142103972.png)

$x_{1}$和$x_2$的皮尔逊相关系数是0.386986

#### (2) 请多次生成数据，观察正则化系数为 1 情况下三种模型拟合参数的稳定性  

生成30次数据。

##### 线性回归

线性回归的斜率和截距的均值及方差

![image-20220424143651018](C:\Users\hutter_sadan\AppData\Roaming\Typora\typora-user-images\image-20220424143651018.png)

线性回归的斜率的均值是3.016 ,方差是0.0098

截距的均值是0.0544，方差是0.835



##### 岭回归

岭回归的斜率和截距的均值及方差

![image-20220424144105251](C:\Users\hutter_sadan\AppData\Roaming\Typora\typora-user-images\image-20220424144105251.png)

岭回归的斜率的均值是2.990 ,方差是0.0136

截距的均值是0.197，方差是0.653

##### lasso回归

lasso回归的斜率和截距的均值及方差

![image-20220424144132836](C:\Users\hutter_sadan\AppData\Roaming\Typora\typora-user-images\image-20220424144132836.png)

lasso回归的斜率的均值是2.940 方差是0.0098

截距的均值是0.0035 方差是0.000353



不难发现，lasso的截距和斜率的方差是最小的，说明lasso对于此题的稳定性最强，岭回归的斜率的稳定性不如线性回归，但是截距的稳定性高于线性回归。



### 10_3

##### 采用fisher法

![image-20220424160420938](C:\Users\hutter_sadan\AppData\Roaming\Typora\typora-user-images\image-20220424160420938.png)

| 特征数   | 训练集上的acc | 测试集上的acc |
| -------- | ------------- | ------------- |
| 1        | 0.88          | 0.89          |
| 5        | 0.957         | 0.96          |
| 10       | 0.967         | 0.96          |
| 20       | 0.97          | 0.95          |
| 50       | 1             | 0.95          |
| 100      | 1             | 0.94          |
| 全部特征 | 1             | 0.93          |

显然，做特征选择后预测结果有所上升。原因主要是通过特征选择减少了过拟合，提高了模型的泛化能力，减少了不必要的特征对于分类的影响。



##### 最大信息系数

![image-20220424171824847](C:\Users\hutter_sadan\AppData\Roaming\Typora\typora-user-images\image-20220424171824847.png)

类似的表：
| 特征数   | 训练集上的acc | 测试集上的acc |
| -------- | ------------- | ------------- |
| 1        | 0.88          | 0.89          |
| 5        | 0.963         | 0.96          |
| 10       | 0.973         | 0.95          |
| 20       | 0.97          | 0.96          |
| 50       | 1             | 0.88          |
| 100      | 1             | 0.91          |
| 全部特征 | 1             | 0.93          |

与fisher法的类似，都有过拟合的样子。

除此之外，请比较两种方法在这些特征个数时挑选出的特征子集有多少特征是相同的 :

先找出选择多少特征的时候效果比较好：可以看出5个特征或者10个特征的时候效果比较好。

在这个特征数下，寻找相同的特征：



|            | 5                      | 10                                              |
| ---------- | ---------------------- | ----------------------------------------------- |
| fisher     | [47, 916, 4, 219, 415] | [47, 916, 219, 4, 415, 476, 271, 224, 825, 461] |
| 最大化信息 | [47, 916, 4, 219, 415] | [47, 916, 219, 4, 415, 835, 468, 407, 747, 634] |

两者在5个特征的时候，选择的特征一模一样，在10个特征的时候，选择的特征只有前5个一样，后面的都不一样。

这说明在选择特征较少时，可能选择的特征是一样的，但是当选择的特征较多时，选择的特征可能会出现差异。



#### 前向算法

1）数据有400个，每一个的都有1000个特征

2）初始化一个空列表$M_{0}$，$M_{i}$表示存放了i个特征。

3）已知$M_{k}$的时候，剩下1000-k个特征，这时候把每个特征分别加入$M_{k}$中，用logisitic回归子啊训练集上训练一次，然后在验证集上看看效果，计算RSS等参数

4）选择RSS最好的特征，与之前的$M_{k}$一起，变成$M_{k+1}$

5）在$M_{1},M_{2},M_{3},,,M_{1000}$中，选择RSS最高的提取出来。



##### 实验结果

当特征选到6个的时候，R^2已经变成了1,继续是实验下去也没有意义了，所以可以直接结束了

| 特征数 | R^2   | 选择的特征              |
| ------ | ----- | ----------------------- |
| 1      | 0.196 | 47                      |
| 2      | 0.464 | 47，4                   |
| 3      | 0.732 | 47，4，103              |
| 4      | 0.866 | 47，4，103，283         |
| 5      | 0.866 | 47，4，103，283，0      |
| 6      | 1     | 47，4，103，283，0，134 |

与上一题的对比，发现差别还是不小的，这前5个选择之中，只有47和4是两者共有的，这说明算法之间还是存在差异的。我认为这里的差异主要是异步算法的问题。无论是fisher还是最大信息系数，都是在同步计算特征的影响，但前向算法是在固定了前面的特征的情况下，观察后续特征的。这可能会造成比较大的选择差异。



##### 决策树算法

![image-20220424200222022](C:\Users\hutter_sadan\AppData\Roaming\Typora\typora-user-images\image-20220424200222022.png)

决策树算法会自动选择特征，用训练集的数据训练决策树，最后选择的特征是

```python
[[47, 0.6331313267176789], [4, 0.1891946068699743], [552, 0.04640269719829376], [916, 0.02853904680203866], [311, 0.02690100430416069], [851, 0.017595625456809486], [117, 0.01743583612306709], [217, 0.012777977044476323], [89, 0.010087876614060258], [856, 0.008967001434720229], [943, 0.008967001434720229]]
```

与（1）相比，共同选取了47，4，916这三个特征。
